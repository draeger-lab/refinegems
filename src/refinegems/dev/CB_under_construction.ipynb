{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brune/miniconda3/envs/sprg/lib/python3.10/site-packages/pydantic/_internal/_config.py:322: UserWarning: Valid config keys have changed in V2:\n",
      "* 'underscore_attrs_are_private' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# ideas for a source test\n",
    "import cobra\n",
    "import pandas as pd\n",
    "\n",
    "import refinegems as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_path = '/Users/brune/Documents/11_Test_Data/test_SPECIMEN/thesis/Kp_std/03_refinement/step4-smoothing/Kp_std_smooth.xml'\n",
    "test_model = rg.io.read_cobra_model(test_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## investigate\n",
    "\n",
    "#### memote testing\n",
    "\n",
    "run memote:\n",
    "- 2 functions, basically doing the same, only differing in output\n",
    "- rather large codeblock for this in SPECIMEN, functions would be nice\n",
    "- SPECIMEN much more verbose than rg\n",
    "\n",
    "function below:\n",
    "- could replace the rg function `run_memote` and `run_memote_sys``\n",
    "- would be usable in SPECIMEN by using `verbose = True` and `save_res`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import memote\n",
    "import time\n",
    "from typing import Literal\n",
    "\n",
    "def run_memote(model: cobra.Model, type:Literal['json','html'], \n",
    "               return_res:bool=False, save_res:str|None=None, verbose:bool=True) -> dict|str|None:\n",
    "\n",
    "    # verbose output I\n",
    "    if verbose:\n",
    "        print('\\n# -------------------\\n# analyse with MEMOTE\\n# -------------------')\n",
    "        start = time.time()\n",
    "\n",
    "    # run memote\n",
    "    ret, res = memote.suite.api.test_model(model, sbml_version=None, results=True,\n",
    "                                           pytest_args=None, exclusive=None, skip=None, \n",
    "                                           experimental=None, solver_timeout=10)\n",
    "    \n",
    "    # load depending on type \n",
    "    match type:\n",
    "        case 'html':\n",
    "            snap = memote.suite.api.snapshot_report(res, html=True)\n",
    "            result = snap\n",
    "        case 'json':\n",
    "            snap = memote.suite.api.snapshot_report(res, html=False)\n",
    "            result = json.loads(snap)\n",
    "        case _:\n",
    "            message = f'Unknown input for parameter how: {type} '\n",
    "            raise ValueError(message)\n",
    "        \n",
    "    # option to save report\n",
    "    if save_res:\n",
    "        with open(save_res, 'w') as f:\n",
    "            f.write(result)\n",
    "\n",
    "    # verbose output II\n",
    "    if verbose:\n",
    "        end = time.time()\n",
    "        print(F'\\ttotal time: {end - start}s')\n",
    "\n",
    "    # option to return report\n",
    "    if return_res:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Info Report\n",
    "\n",
    "- from `inital_analysis` to `get_model_info`\n",
    "    - IDEA: extend and develope into a new report class\n",
    "    - make (some functions) independant of libsbml or cobra, to be used as they are\n",
    "- specimen has similar ideas but a few specific add-ons\n",
    "- below are some ideas on how to do it:\n",
    "    - keep `get_orphans_deadends_disconnected` and `get_mass_charge_unbalanced` in rg\n",
    "    - delete `initial_analysis`, `get_model_info` in rg and `generate_statistics` + the report class in specimen\n",
    "    - use the functions below\n",
    "    - all finished functions (without the TODO label) have been tested and work as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import refinegems as rg\n",
    "from refinegems.reports import Report\n",
    "\n",
    "# @NOTE: \n",
    "#    when sorting the stuff to where it belongs, check imports and co!!!!\n",
    "\n",
    "# @WARNING:\n",
    "#    `get_orphans_deadends_disconnected` and `get_mass_charge_unbalanced` in rg\n",
    "#    are quite reliant on BiGG namespace - also something to rewrite, maybe....\n",
    "\n",
    "# from SPECIMEN extracted\n",
    "# should be added to refinegems, if still of use\n",
    "def get_num_reac_with_gpr(model) -> int:\n",
    "\n",
    "    reac_with_gpr = 0\n",
    "    for reac in model.reactions:\n",
    "        # check for GPR\n",
    "        if len(reac.genes) > 0:\n",
    "            reac_with_gpr += 1\n",
    "\n",
    "    return reac_with_gpr\n",
    "\n",
    "# class for refinegems\n",
    "# @TODO\n",
    "class ModelInfoReport(Report):\n",
    "    \n",
    "    def __init__(self, model) -> None:\n",
    "        \n",
    "        # cobra version\n",
    "        # basics\n",
    "        self.name = model.id\n",
    "        self.reac = len(model.reactions)\n",
    "        self.meta = len(model.metabolites)\n",
    "        self.gene = len(model.genes)\n",
    "        # ends\n",
    "        meta_ordedi = rg.investigate.get_orphans_deadends_disconnected(model)\n",
    "        self.orphans = meta_ordedi[0]\n",
    "        self.deadends = meta_ordedi[1]\n",
    "        self.disconnects = meta_ordedi[2]\n",
    "        # balance\n",
    "        mass_charge = rg.investigate.get_mass_charge_unbalanced(model)\n",
    "        self.mass_unbalanced = mass_charge[0]\n",
    "        self.charge_unbalanced = mass_charge[1]\n",
    "        # gpr\n",
    "        self.with_gpr = get_num_reac_with_gpr(model)\n",
    "\n",
    "    def format_table(self, all_counts=True) -> pd.DataFrame:\n",
    "\n",
    "        data = {'model': [self.name],\n",
    "                '#reactions': [self.reac],\n",
    "                '#metabolites': [self.meta],\n",
    "                '#genes': [self.gene],\n",
    "                'orphans': [', '.join(self.orphans)] if not all_counts else [len(self.orphans)],\n",
    "                'dead-ends': [', '.join(self.deadends)] if not all_counts else [len(self.deadends)],\n",
    "                'disconnects': [', '.join(self.disconnects)] if not all_counts else [len(self.disconnects)],\n",
    "                'mass unbalanced': [', '.join(self.mass_unbalanced)] if not all_counts else [len(self.mass_unbalanced)],\n",
    "                'charge unbalanced': [', '.join(self.charge_unbalanced)] if not all_counts else [len(self.charge_unbalanced)],\n",
    "                '#reactions with gpr': [self.with_gpr]\n",
    "                } \n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    # @TODO\n",
    "    def make_html():\n",
    "        pass\n",
    "\n",
    "    # @TODO\n",
    "    def save(self, dir:str) -> None:\n",
    "\n",
    "        # make sure given directory path ends with '/'\n",
    "        if not dir.endswith('/'):\n",
    "            dir = dir + '/'\n",
    "\n",
    "        # save the statistics report\n",
    "        # ..........................\n",
    "        # @TODO: save as what?\n",
    "        # ..........................\n",
    "\n",
    "\n",
    "# @IDEA\n",
    "# @TODO\n",
    "class MultiModelInfoReport(Report):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # super().__init__()\n",
    "        self.table = pd.DataFrame('model','#reactions','#metabolites',\n",
    "                '#genes','orphans','dead-ends','disconnects','mass unbalanced',\n",
    "                'charge unbalanced','#reactions with gpr')\n",
    "        \n",
    "\n",
    "    def add_single_report(self, report:ModelInfoReport) -> None:\n",
    "        self.table = pd.concat([self.table,report], ignore_index=True)\n",
    "\n",
    "    def __add__(self,other):\n",
    "        self.table = pd.concat(self.table, other.table)\n",
    "\n",
    "    # @TODO\n",
    "    def visualise(self):\n",
    "        pass\n",
    "\n",
    "    # @TODO\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# subclass for SPECIMEN\n",
    "class SpecimenModelInfoReport(ModelInfoReport):\n",
    "    \n",
    "    def __init__(self, model) -> None:\n",
    "\n",
    "        # call the superclass\n",
    "        super().__init__(model)\n",
    "\n",
    "        # find out the origin of the reactions\n",
    "        reac_origin_counts = {'via template':0, 'via MetaNetX':0, 'via KEGG':0, 'via gapfilling':0, 'else':0}\n",
    "        for reac in model.reactions:\n",
    "            # get origin of reaction (based on workflow notation)\n",
    "            if 'creation' in reac.notes.keys():\n",
    "                if reac.notes['creation'] in reac_origin_counts.keys():\n",
    "                    reac_origin_counts[reac.notes['creation']] += 1\n",
    "                else:\n",
    "                    reac_origin_counts['else'] += 1\n",
    "            else:\n",
    "                reac_origin_counts['else'] += 1\n",
    "        \n",
    "        # add new attribute\n",
    "        self.reac_origin_c = reac_origin_counts\n",
    "\n",
    "    # extemd format table function from parent class\n",
    "    def format_table(self) -> pd.DataFrame:\n",
    "        table = super().format_table()\n",
    "        table['#reaction origin'] = str(self.reac_origin_c).replace('{',r'').replace('}',r'').replace('\\'',r'')\n",
    "        return table\n",
    "    \n",
    "    # depending on the implementation, save and make html \n",
    "    # can be inherited or need to be overwritten \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other notes\n",
    "\n",
    "- `parse_reaction`\n",
    "    - seemsa better in entities or io\n",
    "- `get_egc` -> see Tobias\n",
    "    - Note: the dissipation reactions as they are currently used in rg (specimen as well) are hard-coded for BiGG-namespace. Depending on Tobias implementation change this!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## io\n",
    "\n",
    "#### reading in models\n",
    "\n",
    "- transform the three functions used for reading in models into 1\n",
    "- less complicated but would require checking all refinegems and SPECIMEN modules for usage of the function to replace them\n",
    "- functions in question:\n",
    "    - `load_model_cobra`\n",
    "    - `load_model_libsbml`\n",
    "    - `load_multiple_models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libsbml\n",
    "import os\n",
    "\n",
    "def read_model(modelpath: str|list[str], package:Literal['cobra','libsbml']) -> cobra.Model|list[cobra.Model]:\n",
    "\n",
    "    def read_cobra_model(modelpath) -> cobra.Model:\n",
    "\n",
    "        extension = os.path.splitext(modelpath)[1].replace('.','')\n",
    "\n",
    "        match extension:\n",
    "            case 'xml':\n",
    "                data = cobra.io.read_sbml_model(modelpath)\n",
    "            case 'json':\n",
    "                data = cobra.io.load_json_model(modelpath)\n",
    "            case 'yml':\n",
    "                data = cobra.io.load_yaml_model(modelpath)\n",
    "            case 'mat':\n",
    "                data = cobra.io.load_matlab_model(modelpath)\n",
    "            case _:\n",
    "                raise ValueError('Unknown file extension for model: ', extension)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def read_libsbml_model(modelpath) -> libsbml.Model:\n",
    "\n",
    "        reader = libsbml.SBMLReader()\n",
    "        read = reader.readSBMLFromFile(modelpath)  # read from file\n",
    "        mod = read.getModel()\n",
    "\n",
    "        return mod\n",
    "\n",
    "    match modelpath:\n",
    "        # read in multiple models\n",
    "        case list():\n",
    "\n",
    "            loaded_models = []\n",
    "            for modelpath in modelpath:\n",
    "                if package == 'cobra':\n",
    "                    loaded_models.append(read_cobra_model(modelpath))\n",
    "                elif package == 'libsbml':\n",
    "                    loaded_models.append(read_libsbml_model(modelpath))\n",
    "            return loaded_models\n",
    "        \n",
    "        # read in a single model\n",
    "        case str():\n",
    "\n",
    "                if package == 'cobra':\n",
    "                    return read_cobra_model(modelpath)\n",
    "                elif package == 'libsbml':\n",
    "                    return read_libsbml_model(modelpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handling media\n",
    "\n",
    "- couple of functions for handling media based on old database (see below, commented)\n",
    "- basically, all of them can be deleted\n",
    "- question: should the new functions in medium.py that are not part of the Medium class be transferred to io?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pathlib import Path\n",
    "import sqlalchemy\n",
    "import logging\n",
    "\n",
    "from refinegems.io import load_a_table_from_database\n",
    "from refinegems.databases import PATH_TO_DB\n",
    "\n",
    "# multiple options to export media in the Medium class\n",
    "def write_media_to_file(media_file_name: str, media: Union[list[str], str]='all', tsv: bool=True):\n",
    "    \"\"\" Extracts all user-specified media from the database data.db \n",
    "        & Writes them to a CSV/TSV file\n",
    "        Defaults to all media written to a TSV file.\n",
    "\n",
    "    Args:\n",
    "        - media_file_name (str): File name without file extension/Path to file with \n",
    "            file name without file extension\n",
    "        - media (Union[list[str], str], optional): String of medium name/\n",
    "            List of media names. Defaults to 'all'.\n",
    "        - tsv (bool, optional): Specifies if a CSV/TSV file should be returned. \n",
    "            Defaults to True.\n",
    "    \"\"\"\n",
    "    # Generate list of pandas dataframes\n",
    "    media_dfs = []\n",
    "    \n",
    "    # Find out if default should be used\n",
    "    media = load_a_table_from_database('media')['medium'].to_list() if media == 'all' else media\n",
    "    # Turn string input into a list/Sort list of media\n",
    "    if isinstance(media, str): media = [media]\n",
    "    else: media.sort()\n",
    "    # Semi-colon is used for CSV file as ',' can be in substance name\n",
    "    file_sep = '\\t' if tsv else ';'\n",
    "    file_extension = '.tsv' if tsv else '.csv'\n",
    "    \n",
    "    # Iterate over list to get all media pandas dataframes\n",
    "    for medium in media:\n",
    "        medium_df = load_medium_from_db(medium)\n",
    "        media_dfs.append(medium_df)\n",
    "        \n",
    "    requested_media = media_dfs[0] if len(media_dfs) == 1 else pd.concat(media_dfs)\n",
    "    \n",
    "    requested_media.to_csv(f'{media_file_name}{file_extension}', sep=file_sep, \n",
    "                           index=False)\n",
    "\n",
    "\n",
    "# loading external media possible in medium.py\n",
    "# entering a Medium to DB also possible\n",
    "# combined in add_medium()\n",
    "# + more functions to update / extend database\n",
    "def load_custom_media_into_db(mediapath: str) -> pd.DataFrame:\n",
    "    \"\"\" Helper function to read a medium/media definition(s) from a CSV/TSV file \n",
    "        into the database 'data.db' \n",
    "\n",
    "    Args:\n",
    "        - mediapath (str): Path to a .csv/.tsv file containing one or more media \n",
    "            definitions\n",
    "    \"\"\"\n",
    "    # Get file type from file extension\n",
    "    mediapath_filetype = Path(mediapath).suffix\n",
    "    \n",
    "    # Check if file has valid extension/type & get according separator\n",
    "    if mediapath_filetype.lower() == '.csv': seperator = ';'\n",
    "    elif mediapath_filetype.lower() == '.tsv': seperator = '\\t'\n",
    "    else: \n",
    "        logging.error(\n",
    "            'Either no valid file type was provided or the extension of the ' \n",
    "            'file is not one of \\'.tsv\\' or \\'.csv\\'.'\n",
    "            )\n",
    "        return\n",
    "    \n",
    "    custom_media = pd.read_csv(mediapath, sep=seperator)\n",
    "    \n",
    "    # Get table format for media table in database\n",
    "    # Get first column per medium\n",
    "    media_info = custom_media.drop_duplicates(subset=['medium'], keep='first')\n",
    "    # Get fields required for media table\n",
    "    media_info = media_info[['medium', 'medium_description']]\n",
    "    \n",
    "    # Remove for media_compositions table unnecessary column\n",
    "    media_comp = custom_media.drop('medium_description', axis=1)\n",
    "    \n",
    "    # Connect to database\n",
    "    sqlalchemy_engine_input = f'sqlite:///{PATH_TO_DB}'\n",
    "    engine = sqlalchemy.create_engine(sqlalchemy_engine_input)\n",
    "    open_con = engine.connect()\n",
    "    \n",
    "    # Collect existing media to avoid duplicates\n",
    "    existing_media = load_a_table_from_database('media')\n",
    "    \n",
    "    # Remove duplicated media from the DataFrames:\n",
    "    ## 1. Set indeces of the 'media' table from database (existing_media) \n",
    "    ##      & the two dataframes to 'medium'\n",
    "    media_info.set_index('medium', inplace=True)\n",
    "    existing_media.set_index('medium', inplace=True)\n",
    "    media_comp.set_index('medium', inplace=True)\n",
    "    ## 2. Keep all entries in media_info where there is not match in the medium \n",
    "    ##      name compared to the existing_media table\n",
    "    # Get new media for database\n",
    "    media_info = media_info[~media_info.index.isin(existing_media.index)]\n",
    "    ## 3. Keep all entries in media_comp that belong to the new media\n",
    "    media_comp = media_comp[media_comp.index.isin(media_info.index)].reset_index()\n",
    "    # Reset index as only columns are inserted into database\n",
    "    media_info.reset_index(inplace=True)\n",
    "    \n",
    "    # Add new entry/entries for media table first\n",
    "    media_info.to_sql('media', con=open_con, if_exists='append', index=False)\n",
    "    \n",
    "    # Turn medium column into medium_id column\n",
    "    media_comp['medium_query'] = media_comp['medium'].apply(\n",
    "        lambda x: f'SELECT id from media WHERE medium=\\'{x}\\''\n",
    "        ) # Generate SQL query to retrieve link to medium\n",
    "    media_comp['medium_id'] = media_comp['medium_query'].apply(\n",
    "        lambda x: open_con.execute(x).scalar()\n",
    "        ) # Extract medium_id from media table\n",
    "    # Remove for media_compositions table unnecessary columns\n",
    "    media_comp.drop(['medium', 'medium_query'], axis=1, inplace=True)\n",
    "    \n",
    "    # Add new entries for media_compositions table\n",
    "    media_comp.to_sql('media_compositions', con=open_con, if_exists='append', \n",
    "                      index=False)\n",
    "    \n",
    "    # Close connection after insertion\n",
    "    open_con.close()\n",
    "\n",
    "\n",
    "# new one in medium.py\n",
    "def load_medium_from_db(mediumname: str) -> pd.DataFrame:\n",
    "    \"\"\" Helper function to extract subtable for the requested medium from the \n",
    "        database 'data.db'\n",
    "\n",
    "    Args:\n",
    "        - mediumname (str): Name of medium to test growth on\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Table containing composition for one medium with metabs added as BiGG_EX exchange reactions\n",
    "    \"\"\"\n",
    "    medium_query = (\n",
    "        \"SELECT * FROM media m JOIN media_compositions mc ON m.id = \" \n",
    "        f\"mc.medium_id WHERE m.medium = '{mediumname}'\"\n",
    "    )\n",
    "    medium = load_a_table_from_database(medium_query)\n",
    "    medium = medium[['medium', 'medium_description', 'BiGG', 'substance']]\n",
    "    return medium\n",
    "\n",
    "\n",
    "# obsolete, since growth module was restructured and fit to the new database\n",
    "def load_medium_from_db_for_growth(mediumname: str) -> pd.DataFrame:\n",
    "    \"\"\" Wrapper function to extract subtable for the requested medium from the \n",
    "        database 'data.db' & Add the columns 'BiGG_R' and 'BiGG_EX'\n",
    "\n",
    "    Args:\n",
    "        - mediumname (str): Name of medium to test growth on\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Table containing composition for one medium with metabs \n",
    "            added as BiGG_EX exchange reactions\n",
    "    \"\"\"\n",
    "    medium = load_medium_from_db(mediumname)\n",
    "    medium['BiGG_R'] = 'R_EX_' + medium['BiGG'] + '_e'\n",
    "    medium['BiGG_EX'] = 'EX_' + medium['BiGG'] + '_e'\n",
    "    return medium\n",
    "\n",
    "\n",
    "# basically now doable with load_a_table_from_database\n",
    "def load_all_media_from_db(mediumpath: str) -> pd.DataFrame: \n",
    "    \"\"\"Helper function to extract media definitions from media_db.csv\n",
    "\n",
    "    Args:\n",
    "        - mediumpath (str): Path to csv file with medium database\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Table from csv with metabs added as BiGG_EX exchange reactions\n",
    "    \"\"\"\n",
    "    media = pd.read_csv(mediumpath, sep=';')\n",
    "    media['BiGG_R'] = 'R_EX_' + media['BiGG'] + '_e'\n",
    "    media['BiGG_EX'] = 'EX_' + media['BiGG'] + '_e'\n",
    "\n",
    "    media['group'] = media['medium'].ne(media['medium'].shift()).cumsum()\n",
    "    grouped = media.groupby('group')\n",
    "    media_dfs = []\n",
    "    for name, data in grouped:\n",
    "        media_dfs.append(data.reset_index(drop=True))\n",
    "    return media_dfs\n",
    "\n",
    "# additional function to extract a Medium object from a cobra model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### writing models\n",
    "\n",
    "- function `write_to_file` : ambiguous name, should be changes\n",
    "- currently only for writing libsbml model - maybe for cobra as well (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def write_model_to_file(model, filename):\n",
    "\n",
    "    # save cobra model\n",
    "    if isinstance(model, cobra.core.model.Model):\n",
    "        try:\n",
    "            extension = os.path.splitext(filename)[1].replace('.','')\n",
    "            match extension:\n",
    "                case 'xml':\n",
    "                    cobra.io.write_sbml_model(model, filename)\n",
    "                case 'json':\n",
    "                    cobra.io.save_json_model(model, filename)\n",
    "                case 'yml':\n",
    "                    cobra.io.save_yaml_model(model, filename)\n",
    "                case 'mat':\n",
    "                    cobra.io.save_matlab_model(model, filename)\n",
    "                case _:\n",
    "                    raise ValueError('Unknown file extension for model: ', extension)\n",
    "            logging.info(\"Modified model written to \" + filename)\n",
    "        except (OSError) as e:\n",
    "            print(\"Could not write to file. Wrong path?\")\n",
    "\n",
    "    # save libsbml model\n",
    "    elif isinstance(model, libsbml.Model):\n",
    "        try:\n",
    "            new_document = model.getSBMLDocument()\n",
    "            libsbml.writeSBMLToFile(new_document, filename)\n",
    "            logging.info(\"Modified model written to \" + filename)\n",
    "        except (OSError) as e:\n",
    "            print(\"Could not write to file. Wrong path?\")\n",
    "    # unknown model type or no model        \n",
    "    else:\n",
    "        message = f'Unknown model type {type(model)}. Cannot save.'\n",
    "        raise TypeError(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other notes\n",
    "\n",
    "- maybe change name of function `write_report` to something like `write_df_to_excel` to avoid confusion with the reports in the report class as no report is actually save using this function\n",
    "\n",
    "- a bit more ordering in the file would help developers + make it more readable for advanced users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sprg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
